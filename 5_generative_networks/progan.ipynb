{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# --- Uncomment to use only CPU (e.g. GPU memory is too small)\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"/usr/local/cuda-10.1/bin\")\n",
    "# os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda-10.1/lib64\"\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available(cuda_only=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://machinelearningmastery.com/how-to-implement-progressive-growing-gan-models-in-keras/\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Add\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import backend\n",
    "\n",
    "class WeightedSum(Add):\n",
    "    # init with default value\n",
    "    def __init__(self, alpha=0.0, **kwargs):\n",
    "        super(WeightedSum, self).__init__(**kwargs)\n",
    "        self.alpha = backend.variable(alpha, name='ws_alpha')\n",
    "\n",
    "    # output a weighted sum of inputs\n",
    "    def _merge_function(self, inputs):\n",
    "        # only supports a weighted sum of two inputs\n",
    "        assert (len(inputs) == 2)\n",
    "        # ((1-a) * input1) + (a * input2)\n",
    "        output = ((1.0 - self.alpha) * inputs[0]) + (self.alpha * inputs[1])\n",
    "        return output\n",
    "\n",
    "# add a discriminator block\n",
    "def add_discriminator_block(old_model, n_input_layers=3):\n",
    "    # get shape of existing model\n",
    "    in_shape = list(old_model.input.shape)\n",
    "    # define new input shape as double the size\n",
    "    input_shape = (in_shape[-2].value*2, in_shape[-2].value*2, in_shape[-1].value)\n",
    "    in_image = Input(shape=input_shape)\n",
    "    # define new input processing layer\n",
    "    d = Conv2D(64, (1,1), padding='same', kernel_initializer='he_normal')(in_image)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # define new block\n",
    "    d = Conv2D(64, (3,3), padding='same', kernel_initializer='he_normal')(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    d = Conv2D(64, (3,3), padding='same', kernel_initializer='he_normal')(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    d = AveragePooling2D()(d)\n",
    "    block_new = d\n",
    "    # skip the input, 1x1 and activation for the old model\n",
    "    for i in range(n_input_layers, len(old_model.layers)):\n",
    "        d = old_model.layers[i](d)\n",
    "    # define straight-through model\n",
    "    model1 = Model(in_image, d)\n",
    "    # compile model\n",
    "    model1.compile(loss='mse', optimizer=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "    # downsample the new larger image\n",
    "    downsample = AveragePooling2D()(in_image)\n",
    "    # connect old input processing to downsampled new input\n",
    "    block_old = old_model.layers[1](downsample)\n",
    "    block_old = old_model.layers[2](block_old)\n",
    "    # fade in output of old model input layer with new input\n",
    "    d = WeightedSum()([block_old, block_new])\n",
    "    # skip the input, 1x1 and activation for the old model\n",
    "    for i in range(n_input_layers, len(old_model.layers)):\n",
    "        d = old_model.layers[i](d)\n",
    "    # define straight-through model\n",
    "    model2 = Model(in_image, d)\n",
    "    # compile model\n",
    "    model2.compile(loss='mse', optimizer=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "    return [model1, model2]\n",
    "\n",
    "# define the discriminator models for each image resolution\n",
    "def define_discriminator(n_blocks, input_shape=(4,4,3)):\n",
    "    model_list = list()\n",
    "    # base model input\n",
    "    in_image = Input(shape=input_shape)\n",
    "    # conv 1x1\n",
    "    d = Conv2D(64, (1,1), padding='same', kernel_initializer='he_normal')(in_image)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # conv 3x3 (output block)\n",
    "    d = Conv2D(128, (3,3), padding='same', kernel_initializer='he_normal')(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # conv 4x4\n",
    "    d = Conv2D(128, (4,4), padding='same', kernel_initializer='he_normal')(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # dense output layer\n",
    "    d = Flatten()(d)\n",
    "    out_class = Dense(1)(d)\n",
    "    # define model\n",
    "    model = Model(in_image, out_class)\n",
    "    # compile model\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "    # store model\n",
    "    model_list.append([model, model])\n",
    "    # create submodels\n",
    "    for i in range(1, n_blocks):\n",
    "        # get prior model without the fade-on\n",
    "        old_model = model_list[i - 1][0]\n",
    "        # create new model for next resolution\n",
    "        models = add_discriminator_block(old_model)\n",
    "        # store model\n",
    "        model_list.append(models)\n",
    "    return model_list\n",
    "\n",
    "# define models\n",
    "discriminators = define_discriminator(3)\n",
    "# spot check\n",
    "m = discriminators[2][1]\n",
    "m.summary()\n",
    "plot_model(m, to_file='discriminator_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of defining generator models for the progressive growing gan\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import UpSampling2D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Add\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import backend\n",
    "\n",
    "# weighted sum output\n",
    "class WeightedSum(Add):\n",
    "    # init with default value\n",
    "    def __init__(self, alpha=0.0, **kwargs):\n",
    "        super(WeightedSum, self).__init__(**kwargs)\n",
    "        self.alpha = backend.variable(alpha, name='ws_alpha')\n",
    "\n",
    "    # output a weighted sum of inputs\n",
    "    def _merge_function(self, inputs):\n",
    "        # only supports a weighted sum of two inputs\n",
    "        assert (len(inputs) == 2)\n",
    "        # ((1-a) * input1) + (a * input2)\n",
    "        output = ((1.0 - self.alpha) * inputs[0]) + (self.alpha * inputs[1])\n",
    "        return output\n",
    "\n",
    "# add a generator block\n",
    "def add_generator_block(old_model):\n",
    "    # get the end of the last block\n",
    "    block_end = old_model.layers[-2].output\n",
    "    # upsample, and define new block\n",
    "    upsampling = UpSampling2D()(block_end)\n",
    "    g = Conv2D(64, (3,3), padding='same', kernel_initializer='he_normal')(upsampling)\n",
    "    g = BatchNormalization()(g)\n",
    "    g = LeakyReLU(alpha=0.2)(g)\n",
    "    g = Conv2D(64, (3,3), padding='same', kernel_initializer='he_normal')(g)\n",
    "    g = BatchNormalization()(g)\n",
    "    g = LeakyReLU(alpha=0.2)(g)\n",
    "    # add new output layer\n",
    "    out_image = Conv2D(3, (1,1), padding='same', kernel_initializer='he_normal')(g)\n",
    "    # define model\n",
    "    model1 = Model(old_model.input, out_image)\n",
    "    # get the output layer from old model\n",
    "    out_old = old_model.layers[-1]\n",
    "    # connect the upsampling to the old output layer\n",
    "    out_image2 = out_old(upsampling)\n",
    "    # define new output image as the weighted sum of the old and new models\n",
    "    merged = WeightedSum()([out_image2, out_image])\n",
    "    # define model\n",
    "    model2 = Model(old_model.input, merged)\n",
    "    return [model1, model2]\n",
    "\n",
    "# define generator models\n",
    "def define_generator(latent_dim, n_blocks, in_dim=4):\n",
    "    model_list = list()\n",
    "    # base model latent input\n",
    "    in_latent = Input(shape=(latent_dim,))\n",
    "    # linear scale up to activation maps\n",
    "    g  = Dense(128 * in_dim * in_dim, kernel_initializer='he_normal')(in_latent)\n",
    "    g = Reshape((in_dim, in_dim, 128))(g)\n",
    "    # conv 4x4, input block\n",
    "    g = Conv2D(128, (3,3), padding='same', kernel_initializer='he_normal')(g)\n",
    "    g = BatchNormalization()(g)\n",
    "    g = LeakyReLU(alpha=0.2)(g)\n",
    "    # conv 3x3\n",
    "    g = Conv2D(128, (3,3), padding='same', kernel_initializer='he_normal')(g)\n",
    "    g = BatchNormalization()(g)\n",
    "    g = LeakyReLU(alpha=0.2)(g)\n",
    "    # conv 1x1, output block\n",
    "    out_image = Conv2D(3, (1,1), padding='same', kernel_initializer='he_normal')(g)\n",
    "    # define model\n",
    "    model = Model(in_latent, out_image)\n",
    "    # store model\n",
    "    model_list.append([model, model])\n",
    "    # create submodels\n",
    "    for i in range(1, n_blocks):\n",
    "        # get prior model without the fade-on\n",
    "        old_model = model_list[i - 1][0]\n",
    "        # create new model for next resolution\n",
    "        models = add_generator_block(old_model)\n",
    "        # store model\n",
    "        model_list.append(models)\n",
    "    return model_list\n",
    "\n",
    "# define models\n",
    "generators = define_generator(100, 3)\n",
    "# spot check\n",
    "m = generators[2][1]\n",
    "m.summary()\n",
    "plot_model(m, to_file='generator_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
